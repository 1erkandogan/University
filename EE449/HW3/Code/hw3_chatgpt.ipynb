{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment libraries\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.ppo.ppo import PPO\n",
    "from stable_baselines3.dqn.dqn import DQN\n",
    "from gym_super_mario_bros import make\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "# Import preprocessing wrappers\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env.vec_frame_stack import VecFrameStack\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0') # Generates the environment\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT) # Limits the joypads moves with important moves\n",
    "#startGameRand(env)\n",
    "\n",
    "# Apply the preprocessing\n",
    "env = GrayScaleObservation(env, keep_dim=True) # Convert to grayscale to reduce dimensionality\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Alternatively, you may use SubprocVecEnv for multiple CPU processors\n",
    "env = VecFrameStack(env, 4, channels_order='last') # Stack frames\n",
    "env = VecMonitor(env, \"./train/TestMonitor\") # Monitor your progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# PPO training\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mcallback_ppo = SaveOnBestTrainingRewardCallback(save_freq=10000, check_freq=1000, chk_dir=CHECKPOINT_DIR)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mmodel_ppo = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mmodel_ppo.learn(total_timesteps=4000000, callback=callback_ppo)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# Save the best PPO model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m model_ppo \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(CHECKPOINT_DIR, \u001b[39m'\u001b[39;49m\u001b[39mbest_model\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     24\u001b[0m startGameModel(env, model_ppo)\n\u001b[0;32m     25\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# DQN training\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mcallback_dqn = SaveOnBestTrainingRewardCallback(save_freq=10000, check_freq=1000, chk_dir=CHECKPOINT_DIR)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mmodel_dqn = DQN.load(os.path.join(CHECKPOINT_DIR, 'best_model'))\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:730\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m model\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mupdate(data)\n\u001b[0;32m    729\u001b[0m model\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 730\u001b[0m model\u001b[39m.\u001b[39;49m_setup_model()\n\u001b[0;32m    732\u001b[0m \u001b[39m# put state_dicts back in place\u001b[39;00m\n\u001b[0;32m    733\u001b[0m model\u001b[39m.\u001b[39mset_parameters(params, exact_match\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:158\u001b[0m, in \u001b[0;36mPPO._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39msuper\u001b[39m(PPO, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_setup_model()\n\u001b[0;32m    157\u001b[0m \u001b[39m# Initialize schedules for policy/value clipping\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_range \u001b[39m=\u001b[39m get_schedule_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip_range)\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_range_vf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_range_vf, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:91\u001b[0m, in \u001b[0;36mget_schedule_fn\u001b[1;34m(value_schedule)\u001b[0m\n\u001b[0;32m     89\u001b[0m     value_schedule \u001b[39m=\u001b[39m constant_fn(\u001b[39mfloat\u001b[39m(value_schedule))\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(value_schedule)\n\u001b[0;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m value_schedule\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the directories\n",
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR = \"./logs/\"\n",
    "\n",
    "# Create the necessary directories\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize the environment\n",
    "env = make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "env = VecMonitor(env, \"./train/TestMonitor\")\n",
    "'''\n",
    "# PPO training\n",
    "callback_ppo = SaveOnBestTrainingRewardCallback(save_freq=10000, check_freq=1000, chk_dir=CHECKPOINT_DIR)\n",
    "model_ppo = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)\n",
    "model_ppo.learn(total_timesteps=4000000, callback=callback_ppo)\n",
    "'''\n",
    "# Save the best PPO model\n",
    "model_ppo = PPO.load(os.path.join(CHECKPOINT_DIR, 'best_model'))\n",
    "startGameModel(env, model_ppo)\n",
    "'''\n",
    "# DQN training\n",
    "callback_dqn = SaveOnBestTrainingRewardCallback(save_freq=10000, check_freq=1000, chk_dir=CHECKPOINT_DIR)\n",
    "model_dqn = DQN('CnnPolicy', env, batch_size=192, verbose=1, learning_starts=10000,\n",
    "                learning_rate=5e-3, exploration_fraction=0.1, exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.1, train_freq=8, buffer_size=10000, tensorboard_log=LOG_DIR)\n",
    "model_dqn.learn(total_timesteps=4000000, log_interval=1, callback=callback_dqn)\n",
    "\n",
    "# Save the best DQN model\n",
    "model_dqn = DQN.load(os.path.join(CHECKPOINT_DIR, 'best_model'))\n",
    "'''\n",
    "startGameModel(env, model_dqn)\n",
    "\n",
    "# Benchmarking\n",
    "def plot_results(results, labels, ylabel, title):\n",
    "    plt.figure()\n",
    "    for i, result in enumerate(results):\n",
    "        x, y = ts2xy(result, 'timesteps')\n",
    "        plt.plot(x, y, label=labels[i])\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "ppo_results = [load_results(os.path.join(CHECKPOINT_DIR, f'results_ppo_{i}')) for i in range(1)]\n",
    "plot_results(ppo_results, ['PPO_1', 'PPO_2', 'PPO_3'], 'Episode Reward Mean', 'PPO Benchmark')\n",
    "'''\n",
    "dqn_results = [load_results(os.path.join(CHECKPOINT_DIR, f'results_dqn_{i}')) for i in range(3)]\n",
    "plot_results(dqn_results, ['DQN_1', 'DQN_2', 'DQN_3'], 'Episode Reward Mean', 'DQN Benchmark')\n",
    "\n",
    "ppo_vs_dqn_results = [load_results(os.path.join(CHECKPOINT_DIR, f'results_ppo_vs_dqn_{i}')) for i in range(3)]\n",
    "plot_results(ppo_vs_dqn_results, ['PPO vs DQN_1', 'PPO vs DQN_2', 'PPO vs DQN_3'], 'Episode Reward Mean',\n",
    "             'PPO vs DQN Comparison')\n",
    "'''\n",
    "# Change the environment to random stages\n",
    "env_random = make('SuperMarioBrosRandomStages-v0')\n",
    "env_random = JoypadSpace(env_random, SIMPLE_MOVEMENT)\n",
    "env_random = GrayScaleObservation(env_random, keep_dim=True)\n",
    "env_random = DummyVecEnv([lambda: env_random])\n",
    "env_random = VecFrameStack(env_random, 4, channels_order='last')\n",
    "\n",
    "# Start the game with the PPO model in random stages environment\n",
    "startGameModel(env_random, model_ppo)\n",
    "'''\n",
    "# Start the game with the DQN model in random stages environment\n",
    "startGameModel(env_random, model_dqn)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
